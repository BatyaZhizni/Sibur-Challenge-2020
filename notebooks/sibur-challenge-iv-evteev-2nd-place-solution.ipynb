{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sibur Challenge 2020\n",
    "### Задача: \"Прогноз состава сырья\"\n",
    "### Команда: IV & Evteev \n",
    "### 1 место public/2 место private\n",
    "### link: https://sibur.ai-community.com/competitions/4/tasks/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### История и структура решения такая: под конец соревнования уже был готовый пайплайн с препроцессингом, кросс валидацией, и обученим. Но за несколько дней мы с напарником придумали новую систему сдвигов. Переписать все с нуля времени не было. Поэтому большая часть кода дублируется. Сначала считаются тест и трейн после бассейна пропусков в трейне по новым сдвигам, затем отрабатывает старый пайплайн и в него прокидываются новые значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# импорт библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 часть: функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_per_err(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Функция подсчета MAPE\n",
    "    \"\"\"\n",
    "    return (abs((y_true - y_pred) / y_true)).mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "    функция экспоненциального сглаживания\n",
    "    Получаем на вход сериес, и последовательно сглаживаем значения с \"силой\" alpha\n",
    "    \"\"\"\n",
    "    # инициализация новой сериес\n",
    "    result = [series[0]] \n",
    "    \n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "        \n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def A_B_rate_restore(a_rate, b_rate, window, sigma):\n",
    "    \"\"\"\n",
    "    Функция, которая восстанавливает значения A_rate и B_rate\n",
    "    В окне считаем статистики, затем основываясь на среднеквадратичном отклонении определяем аутлаеры\n",
    "    Если после этого у нас не хватает только одного значения из пары A_rate и B_rate, то строим регрессию\n",
    "    И восстанавливаем второе. Если неизвестны оба, заполняем на основе предыдущих значений\n",
    "    \"\"\"\n",
    "    a_rate = a_rate.copy()\n",
    "    b_rate = b_rate.copy()\n",
    "    \n",
    "    # инициализируем новые массивы значений\n",
    "    result_a = [a_rate[0]]\n",
    "    result_b = [b_rate[0]]\n",
    "\n",
    "    # в цикле проходим изначальный массив, заполняя на один шаг вперед основываясь на прошлых данных\n",
    "    for n in range(1, len(a_rate)):\n",
    "        \n",
    "        # статистики по A_rate, считаем среднее значение последних 20 объектов\n",
    "        # находим среднее изменение по последним 100 объектам\n",
    "        # получаем прогноз на следующий элемент, если у нас будет пропуск\n",
    "        mean_a_gup = np.array(result_a[-20:]).mean()\n",
    "        resid_a_gup = (pd.Series(result_a[-100:]) - pd.Series(result_a[-100:]).shift(1)).mean()\n",
    "        mean_a_gup += resid_a_gup\n",
    "        # среднее значение в заданном окне и среднее квадратичное изменение\n",
    "        resid_mean_a = abs(pd.Series(result_a[-window:]) - pd.Series(result_a[-window:]).shift(1)).mean()\n",
    "        resid_std_a = abs(pd.Series(result_a[-window:]) - pd.Series(result_a[-window:]).shift(1)).std()\n",
    "        \n",
    "        # Статистики по A_rate -/-\n",
    "        mean_b_gup = np.array(result_b[-20:]).mean()\n",
    "        resid_b_gup = (pd.Series(result_b[-100:]) - pd.Series(result_b[-100:]).shift(1)).mean()\n",
    "        mean_b_gup += resid_b_gup\n",
    "        \n",
    "        resid_mean_b = abs(pd.Series(result_b[-window:]) - pd.Series(result_b[-window:]).shift(1)).mean()\n",
    "        resid_std_b = abs(pd.Series(result_b[-window:]) - pd.Series(result_b[-window:]).shift(1)).std()\n",
    "        \n",
    "        # для первых значений аутлаеры не ищем, пока не наберется заданое окно значений\n",
    "        if n < window:\n",
    "            pass\n",
    "        else:\n",
    "            # находим аутлаеры, изменение которых сильно больше обычного и заменяем их на Null значение\n",
    "            if abs(a_rate[n] - result_a[n-1]) > resid_mean_a + resid_std_a * sigma:\n",
    "                a_rate[n] = None\n",
    "\n",
    "            if abs(b_rate[n] - result_b[n-1]) > resid_mean_b + resid_std_b * sigma:\n",
    "                b_rate[n] = None\n",
    "                \n",
    "        # если у нового объекта оба значений пропущенны, вставляем срадние значения за предыдущий период\n",
    "        if pd.isnull(a_rate[n]) and pd.isnull(b_rate[n]):\n",
    "            result_a.append(mean_a_gup)\n",
    "            result_b.append(mean_b_gup)\n",
    "            \n",
    "        # если значений A_rate известно\n",
    "        elif pd.isnull(a_rate[n]):\n",
    "            \n",
    "            # строим регрессию по последним 3000 объектов между A_rate и B_rate  \n",
    "            model = Ridge()\n",
    "            model.fit(np.array(result_b[-3000:]).reshape(-1, 1), result_a[-3000:])\n",
    "            result_a.append(model.predict(np.array(b_rate[n]).reshape(-1, 1))[0])\n",
    "            # добавляем известное значение B_rate  \n",
    "            result_b.append(b_rate[n])\n",
    "            \n",
    "        # если значений B_rate известно\n",
    "        elif pd.isnull(b_rate[n]):\n",
    "            \n",
    "            # строим регрессию по последним 3000 объектов между A_rate и B_rate  \n",
    "            model = Ridge()\n",
    "            model.fit(np.array(result_a[-3000:]).reshape(-1, 1), result_b[-3000:])\n",
    "            result_b.append(model.predict(np.array(a_rate[n]).reshape(-1, 1))[0])\n",
    "            # добавляем известное значение A_rate  \n",
    "            result_a.append(a_rate[n])\n",
    "            \n",
    "        # если оба известны, просто добавляем их в новые последовательности\n",
    "        else:\n",
    "            result_a.append(a_rate[n])\n",
    "            result_b.append(b_rate[n])\n",
    "\n",
    "            \n",
    "    return pd.Series(result_a), pd.Series(result_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemical_data_restore(series, window, window_mean, window_resid, sigma):\n",
    "    \"\"\"\n",
    "    Функция, которая восстанавливает значения химических элементов\n",
    "    В окне считаем статистики, затем основываясь на среднеквадратичном отклонении определяем аутлаеры\n",
    "    И на основе статистик заполняем пропуски\n",
    "    \"\"\"\n",
    "    series = series.copy()\n",
    "    \n",
    "    # инициализируем новый массив значений\n",
    "    result = [series[0]]\n",
    "    \n",
    "    # в цикле проходим изначальный массив, заполняя на один шаг вперед основываясь на прошлых данных\n",
    "    for n in range(1, len(series)):\n",
    "        \n",
    "        # статистики по химическому элементу\n",
    "        # находим среднее значение по последним window_mean объектам\n",
    "        mean_gup = np.array(result[-window_mean:]).mean()\n",
    "        # находим среднее изменение по последним window_resid объектам\n",
    "        resid_gup = (pd.Series(result[-window_resid:]) - pd.Series(result[-window_resid:]).shift(1)).mean()\n",
    "        # получаем прогноз на следующий элемент, если у нас будет пропуск\n",
    "        mean_gup += resid_gup\n",
    "        \n",
    "        # среднее значение в заданном окне и среднее изменение \n",
    "        resid_mean = abs(pd.Series(result[-window:]) - pd.Series(result[-window:]).shift(1)).mean()\n",
    "        resid_std = abs(pd.Series(result[-window:]) - pd.Series(result[-window:]).shift(1)).std()\n",
    "        \n",
    "        # для первых значений аутлаеры не ищем, пока не наберется заданое окно значений\n",
    "        if n < window:\n",
    "            pass\n",
    "        else:\n",
    "            # находим аутлаеры, изменение которых сильно больше обычного и заменяем их на Null значение\n",
    "            if abs(series[n] - result[n-1]) > resid_mean + resid_std * sigma:\n",
    "                series[n] = None\n",
    "        \n",
    "        # если пропуск, вставляем расчитаное до этого значение\n",
    "        if pd.isnull(series[n]):\n",
    "            result.append(mean_gup)\n",
    "        else:\n",
    "            result.append(series[n])\n",
    "            \n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_percent(data):\n",
    "    \"\"\"\n",
    "    Функция восстановления процентов состава\n",
    "    После восстановления пропусков некоторые суммы улетели за логичные значения\n",
    "    Больше 100, или меньше 99,2 \n",
    "    Пропорционально восстанавливаем их\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    # находим сумму элементов\n",
    "    data['sum'] = data.iloc[:, 1:-1].T.sum()\n",
    "    # находим кэффициент на который сумма отличается от эталоного значения\n",
    "    data['coef'] = 99.95 / data['sum']\n",
    "    # определяем индексы объектов со сломанной суммой\n",
    "    outlier_indexes = data[(data['sum'] > 99.99) | (data['sum'] < 99.92)].index\n",
    "    \n",
    "    # в цикле пропорционально восстанавливаем значения элементов\n",
    "    for feat in range(1, 9):\n",
    "        data.iloc[outlier_indexes, feat] = data.iloc[outlier_indexes, feat] * data.iloc[outlier_indexes, -1]\n",
    "        \n",
    "    return data.iloc[:, :-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 часть: подготовка данных с новыми адаптивными сдвигами\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных\n",
    "raw_train = pd.read_csv('../data/train_features.csv')\n",
    "raw_test = pd.read_csv('../data/test_features.csv')\n",
    "sample = pd.read_csv('../data/sample_submission.csv')\n",
    "raw_targets = pd.read_csv('../data/train_targets.csv')\n",
    "\n",
    "# удаление временных промежутков, в таргетах он нам нужен для мержда с основным пайплайном\n",
    "raw_train.drop('timestamp', axis=1, inplace=True)\n",
    "raw_test.drop('timestamp', axis=1, inplace=True)\n",
    "# raw_targets.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "# склеиваем все данные вместе\n",
    "data = pd.concat([raw_train, raw_targets], axis=1)\n",
    "data = pd.concat([data, raw_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "# обрезаем начало трейна, так как все что раньше слишком шумно и пробрасываться не будет\n",
    "data = data[2200:].reset_index(drop=True)\n",
    "\n",
    "# разделяем на признаки и на таргеты данные\n",
    "features = data.iloc[:, :10]\n",
    "targets = data.iloc[:, 10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# чистим признаки по выше описаным функциям, некоторые параметры подбирались исходя из структуры изменения признака\n",
    "\n",
    "features['A_rate'], features['B_rate'] = A_B_rate_restore(features['A_rate'], features['B_rate'], 500, 10)\n",
    "\n",
    "features['A_CH4'] = chemical_data_restore(features['A_CH4'], 500, 20, 100, 9)\n",
    "features['A_C2H6'] = chemical_data_restore(features['A_C2H6'], 400, 20, 100, 10)\n",
    "features['A_C3H8'] = chemical_data_restore(features['A_C3H8'], 500, 20, 100, 14)\n",
    "features['A_iC4H10'] = chemical_data_restore(features['A_iC4H10'], 500, 20, 100, 11)\n",
    "features['A_nC4H10'] = chemical_data_restore(features['A_nC4H10'], 500, 20, 100, 11)\n",
    "features['A_iC5H12'] = chemical_data_restore(features['A_iC5H12'], 400, 20, 100, 8)\n",
    "features['A_nC5H12'] = chemical_data_restore(features['A_nC5H12'], 400, 20, 100, 9)\n",
    "features['A_C6H14'] = chemical_data_restore(features['A_C6H14'], 500, 20, 100, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем средний B_rate в скользящем окне\n",
    "features['B_rate_roll'] = features['B_rate'].rolling(190, min_periods=1).mean()\n",
    "\n",
    "\"\"\"\n",
    "Дальше происходит процесс адаптивных сдвигов признаков относительно таргета, так как была найдена\n",
    "взаимосвязь между напором в трубе и временем через которое газ приходит в точку назначения. \n",
    "Обычный сдвиг на константу давал результат хуже. Поэтому в зависимости от напора мы берем более или менее старые данные из прошлого.\n",
    "\"\"\"\n",
    "# инициализируем новые датафреймы\n",
    "new_features = features.copy()\n",
    "new_features.iloc[:, :] = 0\n",
    "\n",
    "new_targets = targets.copy()\n",
    "new_targets.iloc[:, :] = 0\n",
    "\n",
    "# первые 250 значений мусорим, так как не имеем о них качественных значений из прошлого\n",
    "for i in range(features.shape[0]):\n",
    "    if i < 250:\n",
    "        new_features.iloc[i] = features.iloc[0]\n",
    "        new_targets.iloc[i] = targets.iloc[0]\n",
    "    else:\n",
    "        # берем значение скользящего B_rate в момент времени n\n",
    "        b_rate = features.iloc[i]['B_rate_roll']\n",
    "        # расчитываем для него сдвиг, константы были взяты по валидации на тренировочной выборке\n",
    "        # был посчитан средний рейт и оптимальный сдвиг на небольшом куске данных и потом в цикле значения уточнялись \n",
    "        shift = 67/b_rate*198\n",
    "        # исходя из подсчитаного сдвига берем n-shift значение признаков из прошлого\n",
    "        new_features.iloc[i] = features.iloc[int(round(i-shift))]\n",
    "        # таргеты берем в n время\n",
    "        new_targets.iloc[i] = targets.iloc[i]\n",
    "\n",
    "# обрезаем лишние признаки\n",
    "new_features = new_features.iloc[:, :10]\n",
    "new_features = new_features.reset_index(drop=True)\n",
    "new_targets = new_targets.reset_index(drop=True)\n",
    "\n",
    "# обрезаем мусорные 250 значений\n",
    "new_features = new_features[250:].reset_index(drop=True)\n",
    "new_targets = new_targets[250:].reset_index(drop=True)\n",
    "\n",
    "# обрезаем часть тренировочной выборки с аномальными таргетами\n",
    "new_features = new_features.drop(range(2250,2450), axis=0).reset_index(drop=True)\n",
    "new_targets = new_targets.drop(range(2250,2450), axis=0).reset_index(drop=True)\n",
    "\n",
    "# обрезаем часть тренировочной выборки, где таргеты были очень странно сглажены \n",
    "new_features = new_features.drop(range(1290,1440), axis=0).reset_index(drop=True)\n",
    "new_targets = new_targets.drop(range(1290,1440), axis=0).reset_index(drop=True)\n",
    "\n",
    "# сохраняем тестовые значения, 3984 это размер тестовой выборки, отрезаем с конца ее\n",
    "test = new_features[-3984:].reset_index(drop=True)\n",
    "\n",
    "# объединяем трен и таргеты, чтобы удалить пропуски по таргетам, так же у нас удаляться тестовые значений, так как для них таргетов нет\n",
    "data = pd.concat([new_features, new_targets], axis=1).dropna(subset=['B_C2H6', 'B_C3H8', 'B_iC4H10', 'B_nC4H10']).reset_index(drop=True)\n",
    "\n",
    "# добавляем к тренировочным признакам временную метку, и отделяем их от таргетов\n",
    "new_features = data.iloc[:, :11].reset_index(drop=True)\n",
    "new_targets = data.iloc[:, 11:].reset_index(drop=True)\n",
    "\n",
    "# сохраняем тренировочные значения и таргеты\n",
    "train = new_features.copy()\n",
    "train_targets = new_targets.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 часть: основной пайплайн решения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных\n",
    "raw_train = pd.read_csv('../data/train_features.csv')\n",
    "raw_test = pd.read_csv('../data/test_features.csv')\n",
    "raw_targets = pd.read_csv('../data/train_targets.csv')\n",
    "\n",
    "# удаление временных промежутков, в таргетах он нам нужен для мержда с новыми данными\n",
    "raw_train.drop('timestamp', axis=1, inplace=True)\n",
    "raw_test.drop('timestamp', axis=1, inplace=True)\n",
    "# raw_targets.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "# в этом случае использовался константный сдвиг\n",
    "shift = 192\n",
    "full_size = 9792\n",
    "size_test = 3984 + shift\n",
    "size_train = full_size - size_test\n",
    "\n",
    "# объединение данных\n",
    "data = pd.concat([raw_train, raw_targets], axis=1)\n",
    "data = pd.concat([data, raw_test], axis=0)\n",
    "\n",
    "# сдвиг таргетов на фиксированое число\n",
    "data = pd.concat([data.iloc[:, :9].reset_index(drop=True),\n",
    "                  data.iloc[1:, 9].reset_index(drop=True),\n",
    "                  data.iloc[shift:, 10:].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# получение тренировочной, тестовой выборки и таргетов\n",
    "raw_train = data.iloc[:size_train, :10].reset_index(drop=True)\n",
    "raw_targets = data.iloc[:size_train, 10:].reset_index(drop=True)\n",
    "raw_test = data.iloc[size_train:-shift, :10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предварительная чистка трейна\n",
    "\n",
    "data = pd.concat([raw_train, raw_targets], axis=1)\n",
    "\n",
    "# инициализация списка индексов с мусорными объектами\n",
    "trash_indexes = list()\n",
    "# в первых 190 строках очень шумные данные по многим хим элементам\n",
    "trash_indexes += range(0, 190)\n",
    "# # огромный сэмпл пустых/странных значений хим элементов в перемешку с пропусками таргета\n",
    "trash_indexes += range(1191, 1886)\n",
    "# # сэмпл трейна, где таргеты ведут себя очень странно\n",
    "trash_indexes += range(4523, 4689)\n",
    "\n",
    "data = data.drop(trash_indexes, axis=0).reset_index(drop=True)\n",
    "\n",
    "raw_train = data.iloc[:, :10]\n",
    "raw_targets = data.iloc[:, 10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы получить максимально чистые тренировочные данные, до процесса очистки и заполнения пропусков\n",
    "# руками были найденны аутлаеры/пропуски в трейне, которые будут удаленны после восстановления\n",
    "\n",
    "# инициализация списка индексов с мусорными объектами (индексы взяты с учетом предыдущего удаления и сбросом индексов)\n",
    "trash_indexes = list()\n",
    "# пропуски по элементам\n",
    "trash_indexes += raw_train[raw_train['A_C3H8'].isnull()].index.to_list()\n",
    "# # пропуски в таргетах\n",
    "trash_indexes += raw_targets[raw_targets.iloc[:, 1:5].isnull().T.sum() > 0].index.to_list()\n",
    "\n",
    "# аутлаеры по A_rate\n",
    "trash_indexes += range(1131, 1138)\n",
    "trash_indexes += range(2822, 2828)\n",
    "trash_indexes += [4032]\n",
    "trash_indexes += range(4150, 4154)\n",
    "trash_indexes += range(4214, 4217)\n",
    "trash_indexes += range(4345, 4348)\n",
    "\n",
    "# аутлаеры по B_rate\n",
    "trash_indexes += range(2110, 2112)\n",
    "trash_indexes += range(4492, 4495)\n",
    "\n",
    "# аутлаеры по A_CH4\n",
    "trash_indexes += [3850]\n",
    "trash_indexes += range(843, 871)\n",
    "trash_indexes += range(1250, 1254)\n",
    "\n",
    "# аутлаеры по A_C2H6\n",
    "trash_indexes += range(3850, 3855)\n",
    "trash_indexes += range(2829, 2831)\n",
    "trash_indexes += range(4348, 4350)\n",
    "trash_indexes += range(1475, 1478)\n",
    "\n",
    "# аутлаеры по A_C3H8\n",
    "trash_indexes += range(3534, 3539)\n",
    "trash_indexes += range(3578, 3586)\n",
    "\n",
    "# аутлаеры по таргетам\n",
    "trash_indexes += range(3638, 3641)\n",
    "trash_indexes += range(1341, 1349)\n",
    "trash_indexes += range(2835, 2837)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# восстановливаем только тренировочные данные, так как тестовые целиком будут взяты из данных с новыми сдвигами\n",
    "\n",
    "raw_train['A_rate'], raw_train['B_rate'] = A_B_rate_restore(raw_train['A_rate'], raw_train['B_rate'], 1000, 12)\n",
    "\n",
    "raw_train['A_CH4'] = chemical_data_restore(raw_train['A_CH4'], 500, 20, 100, 10)\n",
    "raw_train['A_C2H6'] = chemical_data_restore(raw_train['A_C2H6'], 500, 20, 100, 14)\n",
    "raw_train['A_C3H8'] = chemical_data_restore(raw_train['A_C3H8'], 500, 20, 100, 15)\n",
    "raw_train['A_iC4H10'] = chemical_data_restore(raw_train['A_iC4H10'], 500, 20, 100, 11)\n",
    "raw_train['A_nC4H10'] = chemical_data_restore(raw_train['A_nC4H10'], 500, 20, 100, 11)\n",
    "raw_train['A_iC5H12'] = chemical_data_restore(raw_train['A_iC5H12'], 500, 20, 100, 7)\n",
    "raw_train['A_nC5H12'] = chemical_data_restore(raw_train['A_nC5H12'], 400, 20, 100, 9)\n",
    "raw_train['A_C6H14'] = chemical_data_restore(raw_train['A_C6H14'], 500, 20, 100, 18)\n",
    "\n",
    "# восстановление процентов\n",
    "raw_train = restore_percent(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные для предсказания\n",
    "final_train = raw_train.copy()\n",
    "final_targets = raw_targets.copy()\n",
    "\n",
    "# удаляем ранее отмеченные аутлаеры и пропуски\n",
    "data = pd.concat([final_train, final_targets], axis=1)\n",
    "data.drop(list(set(trash_indexes)), axis=0, inplace=True)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# отделяем трейн и таргеты\n",
    "final_train = data.iloc[:, :10].copy()\n",
    "final_targets = data.iloc[:, 10:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# еще щепотка чистки трейна\n",
    "data = pd.concat([final_train, final_targets], axis=1) \n",
    "data = data.drop([1353, 1354, 1355, 1437], axis=0).reset_index(drop=True)\n",
    "\n",
    "# переносим метку времени в трейн из таргетов для последующего объединения по ней\n",
    "# и поэтому делим [:, :11], а не [:, :10], как до этого\n",
    "final_train = data.iloc[:, :11]\n",
    "final_targets = data.iloc[:, 11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как было упомянуто в начале ноутбука. \n",
    "# На данный момент мы имеем сэмпл трейна и тест посчитанный с новыми сдвигами\n",
    "# И трейн посчитаный со старыми константными сдвигами\n",
    "# И сейчас мы заменим старые данные на новые по тем объектам для которых смогли посчитать объекты с новыми сдвигами\n",
    "\n",
    "# по временной метке мерджим старые и новые данные\n",
    "comb_data = pd.merge(final_train, train, on='timestamp', how='left')\n",
    "# выделяем индексы для которых у нас есть новые значения\n",
    "idx = comb_data[~comb_data['A_rate_y'].isnull()].index\n",
    "# заменяем часть трейна на новые данные\n",
    "comb_train = final_train.iloc[:, :10].copy()\n",
    "comb_train.iloc[idx] = comb_data[~comb_data['A_rate_y'].isnull()].iloc[:, 11:]\n",
    "# сохраняем тренировочную выборку\n",
    "final_train = comb_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оверсэмплинг трейна\n",
    "# для сглаживания выборки, берем трейн, через один восстанавливаем пропуски и интерполируем\n",
    "\n",
    "final_data = pd.concat([final_train, final_targets], axis=1)\n",
    "\n",
    "# инициализация нового датафрейма\n",
    "valid_data = pd.DataFrame()\n",
    "# cчетчик\n",
    "j = 0\n",
    "# пустая строка для вставки\n",
    "none = pd.Series([None]*14).T\n",
    "\n",
    "# проходим в цикле и вставляем пустую строку между двух известных значений\n",
    "for i in range(final_data.shape[0]*2-1):\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        valid_data = valid_data.append(final_data.iloc[j])\n",
    "        j += 1\n",
    "    else:\n",
    "        valid_data = valid_data.append(none, ignore_index=True)\n",
    "        \n",
    "# отрезаем появившиеся лишние признаки\n",
    "valid_data = valid_data.iloc[:, :14]\n",
    "# восстанавливаем имена столбцов\n",
    "valid_data = valid_data[final_data.columns]\n",
    "# интерполируем\n",
    "valid_data = valid_data.interpolate()\n",
    "valid_data = valid_data.reset_index(drop=True)\n",
    "\n",
    "final_data = valid_data.copy()\n",
    "\n",
    "\n",
    "# финальные трейн и таргеты\n",
    "final_train = final_data.iloc[:, :10]\n",
    "final_targets = final_data.iloc[:, 10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# финальный тест\n",
    "final_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кастомная кросс валидация на 16 фолдов\n",
    "# инициализируем первый фолд как первые 912 значений, и предсказываем весь трейн\n",
    "# дальше проходим окном с шагом 456 по всему трейну\n",
    "# получаем 16 предсказаний трейна, которые усредняем\n",
    "cv = [[np.arange(0 + i * 456, 912 + i * 456), np.arange(0, 8215)] for i in range(16)]\n",
    "\n",
    "# в попытках настроить стабильную валидацию, некоторые фолды в итоге не использовались\n",
    "drop_folds = [0, 7, 3, 8, 13]\n",
    "cv = [cv[i] for i in range(len(cv)) if i not in drop_folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализация датафрейма с предсказаниями теста\n",
    "submission = sample.copy()\n",
    "submission.iloc[:, 1:] = 0\n",
    "\n",
    "# сумма ошибки на валидации\n",
    "total_loss = 0\n",
    "\n",
    "# цикл для предсказания каждого таргета отдельно\n",
    "for num, target in enumerate(final_targets.columns):\n",
    "    \n",
    "    print(target, end='  ')\n",
    "    \n",
    "    # переменная для сохранения ошибки\n",
    "    loss = 0\n",
    "    # сериес для запись результата предсказания\n",
    "    res = pd.Series(np.zeros(final_train.shape[0]))\n",
    "    \n",
    "    # инициализируются фоллды\n",
    "    for num, (train_idx, test_idx) in enumerate(cv):\n",
    "        \n",
    "        # разбиение на тестовую и тренировочную выборку внутри трейна\n",
    "        x_train = final_train.iloc[train_idx]\n",
    "        x_test = final_train.iloc[test_idx]\n",
    "        y_train = final_targets.iloc[train_idx][target]\n",
    "        y_test = final_targets.iloc[test_idx][target].reset_index(drop=True)\n",
    "\n",
    "        # инициализируем модель\n",
    "        model = Ridge()\n",
    "        # обучаем модель\n",
    "        model.fit(x_train, y_train)\n",
    "        # предсказываем значения для валидации\n",
    "        res += model.predict(x_test) / len(cv)\n",
    "        # предсказываем значения для теста\n",
    "        submission[target] += model.predict(final_test) / len(cv)\n",
    "    \n",
    "    # экспоненциальное сглаживание таргетов, коэфициенты подобраны на валидации\n",
    "    if target == 'B_C2H6':\n",
    "        res = exponential_smoothing(res, 0.65)\n",
    "        submission[target] = exponential_smoothing(submission[target], 0.65)\n",
    "        \n",
    "    if target == 'B_C3H8':\n",
    "        res = exponential_smoothing(res, 0.2)\n",
    "        submission[target] = exponential_smoothing(submission[target], 0.2)\n",
    "        \n",
    "    if target == 'B_iC4H10':\n",
    "        res = exponential_smoothing(res, 1)\n",
    "        submission[target] = exponential_smoothing(submission[target], 1)\n",
    "        \n",
    "    if target == 'B_nC4H10':\n",
    "        res = exponential_smoothing(res, 0.35)\n",
    "        submission[target] = exponential_smoothing(submission[target], 0.35)\n",
    "        \n",
    "    # ошибка по таргету\n",
    "    loss = mean_abs_per_err(y_test, res)\n",
    "    \n",
    "    total_loss += loss \n",
    "    print(round(loss, 5))\n",
    "\n",
    "print()\n",
    "print('total_loss', round(total_loss, 5) / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../submissions/combinated_version_v_6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
